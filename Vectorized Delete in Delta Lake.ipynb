{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036ffe33-5806-43bd-ab44-18b326f6483b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vectorized Delete in Delta Lake: A Performance Game-Changer\n",
    "\n",
    " \"\uD83D\uDE80 Delta Lake's Vectorized Delete: The Secret to 10x Faster Data Operations!\"\n",
    "\n",
    "\n",
    "\n",
    "Big news for data engineers! Delta Lake 2.0+ introduces vectorized delete - a revolutionary optimization that dramatically improves DELETE operation performance.\n",
    "\n",
    "\uD83D\uDD0D What is it?\n",
    "Vectorized delete processes multiple records in a single operation rather than row-by-row, reducing I/O operations and improving throughput.\n",
    "\n",
    "\uD83D\uDCA1 Why it matters:\n",
    "\n",
    "10x faster DELETE operations on large datasets\n",
    "\n",
    "Reduced compute costs\n",
    "\n",
    "Lower latency for critical data pipelines\n",
    "\n",
    "When to use it:\n",
    "✅ Bulk deletion of stale records\n",
    "✅ GDPR/compliance data purging\n",
    "✅ Regular data maintenance operations\n",
    "\n",
    "Pro tip: Combine with OPTIMIZE and ZORDER for maximum performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0df5e29-2044-4b5b-af46-0aa33ad29eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Generate Sample Data (100M Records)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3944192a-dce6-484f-b4a0-0f9d2bd58e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switching to the analytics catalog\n",
    "USE CATALOG analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d787c04-7688-46c2-99e5-b4606a517304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switching to the customer_360 schema in the analytics catalog\n",
    "USE analytics.customer_360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd7cdd0-80d2-4d90-aa4c-52e51b494fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS analytics.customer_360.customers\")\n",
    "\n",
    "# Generate synthetic customer data (100M records)\n",
    "df = spark.range(0, 100_000_000).selectExpr(\n",
    "    \"id as customer_id\",\n",
    "    \"concat('user_', id) as name\",\n",
    "    \"date_add(current_date(), -cast(rand() * 1000 as int)) as last_purchase_date\",\n",
    "    \"cast(rand() * 1000 as int) as purchase_count\"\n",
    ")\n",
    "\n",
    "# Write the DataFrame as a Delta table, automatically registering it in the metastore\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"analytics.customer_360.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2637073-8dbb-4dc3-ae55-e86cd4655af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Traditional DELETE (Row-by-Row)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020fc9ab-3481-4bcf-8670-d3e1da074324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mTraditional DELETE took: 4.60 seconds\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import time  # Ensure you import the time module\n",
    "\n",
    "# Benchmark DELETE operation\n",
    "start_time = time.time()\n",
    "\n",
    "# Replace 'customers' with your fully qualified table name if required\n",
    "spark.sql(\"DELETE FROM customers WHERE last_purchase_date < date_sub(current_date(), 365)\")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\033[1mTraditional DELETE took: {execution_time:.2f} seconds\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b330951-8ee1-4959-b285-2403eff7d0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**deletion vectors on the table **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b4fafb-b74e-4d0c-adb7-1916633f9cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[4mDELETE with Deletion Vectors took: 1.13 seconds\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# Enable deletion vectors on the table \n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE analytics.customer_360.customers \n",
    "  SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\n",
    "\"\"\")\n",
    "\n",
    "# Benchmark DELETE operation using deletion vectors\n",
    "start_time = time.time()\n",
    "\n",
    "# Update the table name if necessary; this DELETE will now use deletion vectors\n",
    "spark.sql(\"\"\"\n",
    "  DELETE FROM analytics.customer_360.customers \n",
    "  WHERE last_purchase_date < date_sub(current_date(), 365)\n",
    "\"\"\")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\033[1m\\033[4mDELETE with Deletion Vectors took: {execution_time:.2f} seconds\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e30cb9-0b8a-4023-927b-eb767c48941b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When you run the DELETE operation with deletion vectors enabled, Delta Lake avoids the typical heavy copy-on-write rewrite of entire data files. Instead, it performs these steps behind the scenes:\n",
    "- Identifying Rows to Delete:\n",
    "The DELETE command evaluates your condition (in this case, last_purchase_date < date_sub(current_date(), 365)), determining which rows should be removed.\n",
    "- Recording Deletions Via Vectors:\n",
    "Instead of rewriting all affected files, Delta Lake creates or updates “deletion vectors.” These are lightweight metadata constructs—often in the form of bitmaps or lists—that mark the specific rows in a file as deleted. Essentially, the original data file remains intact, but Delta records offsets or row positions that should be skipped in future reads.\n",
    "\n",
    "- Query-Time Row Filtering:\n",
    "When queries read the table, Delta automatically applies these deletion vectors, filtering out the logically deleted rows. This means that from a query perspective, the deleted rows are invisible, even though the underlying file hasn’t been physically rewritten.\n",
    "- Deferred Maintenance:\n",
    "Over time (for example, during an OPTIMIZE operation), Delta may compact data and physically remove the rows marked as deleted. This consolidates the deletion vectors and reclaims storage, but that’s a background maintenance step separate from your immediate DELETE command.\n",
    "\n",
    "So, in your benchmark code, when you issue the DELETE command on the table with deletion vectors enabled, Delta Lake performs a lightweight, vectorized removal of rows, which is typically much faster than the standard full file rewrite—especially when only a small subset of rows needs to be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2f46b9-8db4-477c-bbaa-e64d5a7d49a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6468156313777475,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Vectorized Delete in Delta Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}